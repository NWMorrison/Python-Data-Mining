
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Midterm</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; columns:4; font-size: 8px;}
        pre { background-color: #f4f4f4; padding: 10px; border-radius: 5px;}
    </style>
</head>
<body>
<pre>
import numpy as np
list =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
array1 = np.array([[1,2,3],[4,5,6]])
# list[2:]       = [2, 3, 4, 5, 6, 7, 8, 9]
# list[:4]       = [0, 1, 2, 3]
# list[-2]       = 8
# list[:-2]      = [0, 1, 2, 3, 4, 5, 6, 7]
# list[-4:]      = [6, 7, 8, 9]
# list[3:6]      = [3, 4, 5]
# list[-5:-2]    = [5, 6, 7]
# list[::-1]     = [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
# list[::2]      = [0, 2, 4, 6, 8]
# list[1::2]     = [1, 3, 5, 7, 9]
# list[-2:-5:-1] = [8, 7, 6]
# list[5:3:-1]   = [5, 4]
# list[-1:-8:-3] = [9, 6, 3]
# array1[0,:]    = [1 2 3]
# array1[:,0]    = [1 4]
# array1[-1]     = [4 5 6]
# array[1, :2]   = [4 5]
print(array1)
</pre>

<pre>
## Broadcasting
import numpy as np
i = np.array([[13,5,2], [7,14,9], [1, 11, 2]])
j = np.array([1, 2, 1])
add = i + j
mult = i * j
div = i / j
sub = i - j
print(add, "\n")
print(mult, "\n")
print(div, "\n")
print(sub)

C = np.array ([[2,5,1],
              [4,10,2],
              [-3,5,11]])
</pre>

<pre>
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
df_iris = pd.read_csv('/content/iris.csv', 
							 header=None)
df_iris.columns = ['x1', 'x2', 'x3', 'x4', 'labels']
## This turns our 'labels' into 
#   an actual array out to console.
df_iris['labels'].to_numpy()
# Provides to rows & columns (150,5)
print(df_iris.shape)
# Just provides the length of the Dataframe 150
print(len(df_iris))
# it is better to use loc or iloc, see documentation
df_iris.iloc[5:10]
</pre>

<pre>
#using pandas dataframes and map()
colors = {1:'red', 2:'green', 3:'blue'}
plt.scatter(df_iris['x1'], df_iris['x3'], 
			20, df_iris['labels']
			.map(colors))
plt.title('Iris Dataset')
plt.xlabel('x1')
plt.ylabel('x3')
plt.show()
</pre>

<pre>
df_iris.info()
</pre>
<img src="code5.png" width="200" height="200">

<pre>
df_iris.describe()
</pre>

<img src="code6.png" width="250" height="200">

<pre>
## see the matplot lib documentation 
#   for colors, markers, plot()
condition1 = labels==1
condition2 = labels==2
condition3 = labels==3
plt.plot(iris[condition1,2], iris[condition1,3], 'bx')
plt.plot(iris[condition2,2], iris[condition2,3], 'g+')
plt.plot(iris[condition3,2], iris[condition3,3], 'r.')
plt.title('Iris Dataset')
plt.xlabel('x2')
plt.ylabel('x3')
plt.show()
</pre>
<br>
<br>
<pre>
## K-Means Steps
from sklearn.cluster import KMeans
import numpy as np
# df = pd.read_csv(...)
# df.columns = ['x1', 'x2']
data = df.to_numpy()

# change this to enter different number of clusters
k = 5

# call k-means fit() to apply clustering on the data
kmeans = KMeans(n_clusters=k, 
		       random_state=40, 
			   n_init="auto")
			   .fit(data)

## results: cluster centroids, 
#   the centroid for each cluster

kmeans.cluster_centers_

"""
array([[3.98854286, 5.64302857],
       [6.63423333, 3.1914    ],
       [2.86582857, 7.69724286],
       [7.31955   , 2.44526667],
       [3.15572   , 6.69342   ]])
"""


# cluster labels assigned to each point in the data
kmeans.labels_

"""
array([2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 0, 
	   0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1,
       1, 3, 4, 1, 3, 3, 3, 3, 3], dtype=int32)
"""

from sklearn.metrics import metrics
from sklearn.metrics import pairwise_distances
metrics.silhouette_score(data, 
			   kmeans.labels_, 
			   metric='euclidean')

## total SSE[Sum-Squared-Error], 
#  over ALL the clusters and data points,
# they call it inertia
kmeans.inertia_
</pre>

<pre>
## K-NN
# Split the data into train set and test set
# we need to do that in order to first train 
# the model with the train data and
# labels and then test with the test data.
# our goal is to see how well it predicts 
# the test labels based on the model.
# This split is 80-20 train-test, 
# you can adjust it in the test_size parameter.
# The random state is to give the 
# same seed for the random split

from sklearn.model_selection import train_test_split
import numpy as np

labels = df['labels'].to_numpy()
data = (df.drop('labels', axis=1)).to_numpy()
# X_train: the train data points
# X_test: the test data points
# y_train: the labels for the train points
# y_test: the labels for the test points
X_train, X_test, y_train, y_test = train_test_split 
		= (data,
           labels,
           test_size=0.2,
           random_state=42)

from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)

# Fit the model to our training data and their labels
# for this use X_train (train data) 
# and y_train (train labels)
# then use the model.fit() function
model.fit(X_train, y_train)
# Predict the class (labels) of the unseen test data
# the y_pred variable will hold the predicted labels
# here use the model.predict() function and the
# X_test (the test set data points)
y_pred = model.predict(X_test)
</pre>

<pre>
## Confusion Matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

# Calculate the confusion matrix
# This line calculates the confusion 
# matrix using the true labels (y_test) and
# the predicted labels (y_pred). 
# The confusion matrix shows how many samples
# were correctly and incorrectly classified 
# for each class.
cm = confusion_matrix(y_test, y_pred)
"""
array([[10,  0,  0],
       [ 0,  8,  0],
       [ 0,  1, 11]])
"""
## We are looking at the main diagonal element 
#  across the matrix.
# [TLeft->BRight]
# Those represent correct predictions for that class. 
#  The off diagonal elements
#   represent misclassifications. 
#  If this number is high, our model is constantly
#   confusing certain classes with each other.
# Seems to be classifying the data quite well.

accuracy_score(y_test, y_pred)

# Calculates and prints our accuracy by sum() call
accuracy = np.sum(np.diag(cm)) / np.sum(cm)
print(f"Accuracy: {accuracy:.4f}\n")

## Standardize features by removing the mean and scaling to 
#   unit variance.
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
# Fit only on X_train
scaler.fit(X_train)

# Scale both X_train and X_test
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
</pre>

</body>
</html>
